Below is a **comprehensive GitHub repository structure and README** for your Autonomous Perception System based on your uploaded `av_perception_system.py` implementation.

This includes:

* Full project architecture
* Flow diagram
* Mathematical formulation
* Model sources & citations
* Pseudo-LiDAR equations
* 3D box projection math
* Kalman filtering equations
* BEV transformation math
* Installation instructions
* Research-grade documentation

---

# ğŸš— AV-Multimodal-Perception-System

> A modular autonomous perception stack combining detection, tracking, segmentation, monocular depth estimation, pseudo-LiDAR reconstruction, stabilized 3D bounding boxes, and BEV mapping.

---

# ğŸ“ Repository Structure

```
AV-Multimodal-Perception-System/
â”‚
â”œâ”€â”€ av_perception_system.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ yolo/
â”‚   â”œâ”€â”€ yolop/
â”‚   â”œâ”€â”€ sam2/
â”‚   â”œâ”€â”€ depth_anything/
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ deepsort.yaml
â”‚   â”œâ”€â”€ camera_intrinsics.yaml
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ bev_topdown.ply
â”‚   â”œâ”€â”€ pointcloud_with_boxes.ply
â”‚   â”œâ”€â”€ textured_pointcloud.ply
â”‚
â”œâ”€â”€ depth_log.csv
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ docs/
    â”œâ”€â”€ system_diagram.png
    â”œâ”€â”€ equations.md
```

---

# ğŸ§  System Overview

This system integrates:

| Module                            | Model Used                          |
| --------------------------------- | ----------------------------------- |
| Object Detection                  | YOLOv9                              |
| Object Tracking                   | DeepSORT                            |
| Drivable Area + Lane Segmentation | YOLOP                               |
| Instance Segmentation             | SAM 2                               |
| Depth Estimation                  | Depth Anything V3 (DA3Metric-Large) |
| 3D Box Stabilization              | Kalman Filter                       |
| Pseudo-LiDAR BEV                  | Custom EnhancedPseudoLidarBEV       |
| 3D Point Cloud Export             | Open3D                              |

---

# ğŸ” System Flow Diagram

```
                RGB Frame
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   YOLOv9         â”‚
         â”‚  Object Detect   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   DeepSORT       â”‚
         â”‚   Tracking       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Depth Anything   â”‚
         â”‚ Monocular Depth  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Pseudo-LiDAR     â”‚
         â”‚ 3D Reconstructionâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ 3D Box Estimator â”‚
         â”‚ + Kalman Filter  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ BEV Renderer     â”‚
         â”‚ + PLY Export     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“ Mathematical Formulation

---

## 1ï¸âƒ£ Camera Projection Model

Camera intrinsic matrix:

[
K =
\begin{bmatrix}
f_x & 0 & c_x \
0 & f_y & c_y \
0 & 0 & 1
\end{bmatrix}
]

Pixel to 3D camera coordinate:

[
X = \frac{(u - c_x) Z}{f_x}
]

[
Y = \frac{(v - c_y) Z}{f_y}
]

[
Z = \text{Depth value}
]

Used in:

* `pixel_to_camera_coords()`
* `depth_to_bev_points()`
* 3D box projection

---

## 2ï¸âƒ£ Monocular Depth Estimation

Using:

* Depth Anything V3 (metric model)

Depth inference:

[
D = f_\theta(I)
]

Where:

* ( I ) = RGB image
* ( \theta ) = pretrained transformer weights
* ( D ) = dense depth map

---

## 3ï¸âƒ£ Pseudo-LiDAR Reconstruction

Convert depth map to 3D points:

[
\mathbf{P} = (X, Y, Z)
]

Filter by:

[
0.1 < Z < 80m
]

Points stacked into rolling buffer:

[
P_{global} = \bigcup_{t-k}^{t} P_t
]

---

## 4ï¸âƒ£ BEV Transformation

Top-down projection:

[
x_{bev} = center_x + X \cdot scale
]

[
y_{bev} = center_y - Z \cdot scale
]

Height-based coloring:

[
color = \text{colormap}(Y)
]

---

## 5ï¸âƒ£ 3D Bounding Box Estimation

Vehicle dimensions:

[
w, h, l
]

3D corners:

[
C = \begin{bmatrix}
\pm w/2 & 0 & \pm l/2 \
\pm w/2 & -h & \pm l/2
\end{bmatrix}
]

Yaw rotation:

[
R_y =
\begin{bmatrix}
\cos \psi & 0 & \sin \psi \
0 & 1 & 0 \
-\sin \psi & 0 & \cos \psi
\end{bmatrix}
]

Projection:

[
p_{2D} = K \cdot (R_y C + T)
]

---

## 6ï¸âƒ£ Kalman Filter for Yaw

State:

[
x =
\begin{bmatrix}
\psi \
\dot{\psi}
\end{bmatrix}
]

Prediction:

[
x_{k|k-1} = A x_{k-1}
]

Where:

[
A =
\begin{bmatrix}
1 & 1 \
0 & 1
\end{bmatrix}
]

Measurement update:

[
x_{k|k} = x_{k|k-1} + K (z_k - H x_{k|k-1})
]

---

## 7ï¸âƒ£ Position Kalman

State:

[
[x, y, \dot{x}, \dot{y}]
]

Used to stabilize bottom anchor of 3D box.

---

# ğŸ“¦ Model Sources

---

## ğŸ”¹ YOLOv9

Paper:

> Wang et al., YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information (2024)

Repo:
[https://github.com/WongKinYiu/yolov9](https://github.com/WongKinYiu/yolov9)

---

## ğŸ”¹ DeepSORT

Paper:

> Wojke et al., Simple Online and Realtime Tracking (2017)

Repo:
[https://github.com/ZQPei/deep_sort_pytorch](https://github.com/ZQPei/deep_sort_pytorch)

---

## ğŸ”¹ YOLOP

Paper:

> YOLOP: You Only Look Once for Panoptic Driving Perception (2021)

Repo:
[https://github.com/hustvl/YOLOP](https://github.com/hustvl/YOLOP)

---

## ğŸ”¹ SAM 2

Meta AI:

> Segment Anything Model 2

Repo:
[https://github.com/facebookresearch/segment-anything-2](https://github.com/facebookresearch/segment-anything-2)

Ultralytics integration:
[https://docs.ultralytics.com](https://docs.ultralytics.com)

---

## ğŸ”¹ Depth Anything V3

Paper:

> Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data (2024)

Model:
depth-anything/DA3METRIC-LARGE

Repo:
[https://github.com/DepthAnything/Depth-Anything-V2](https://github.com/DepthAnything/Depth-Anything-V2)
[https://github.com/DepthAnything/Depth-Anything-3](https://github.com/DepthAnything/Depth-Anything-3)

---

## ğŸ”¹ Intel DPT

Paper:

> Vision Transformers for Dense Prediction (DPT)

Repo:
[https://github.com/isl-org/DPT](https://github.com/isl-org/DPT)

---

## ğŸ”¹ Open3D

Library:
[http://www.open3d.org/](http://www.open3d.org/)

---

# ğŸš€ Installation

```bash
git clone https://github.com/yourname/AV-Multimodal-Perception-System
cd AV-Multimodal-Perception-System

pip install -r requirements.txt
```

If using Depth Anything V2:

```bash
git clone https://github.com/DepthAnything/Depth-Anything-V2
```

---

# â–¶ï¸ Run

```bash
python av_perception_system.py --source video.mp4
```

---

# ğŸ“¤ Outputs

* BEV overlay video
* `bev_topdown.ply`
* `pointcloud_with_boxes.ply`
* `textured_pointcloud.ply`
* `depth_log.csv`

---

# ğŸ”¬ Research Contributions

* Rolling pseudo-LiDAR buffer
* Adaptive 3D bounding box scaling
* EMA + Kalman fusion stabilization
* BEV semantic recoloring
* Exportable 3D scene reconstruction

---

# ğŸ“ˆ Future Extensions

* Multi-frame SLAM fusion
* Ego-motion compensation
* Multi-camera fusion
* Radar/LiDAR sensor fusion
* Transformer-based 3D box estimation
* Town-scale mapping expansion

---

# ğŸ Conclusion

This repository implements a **full perception stack approximating production autonomous driving pipelines**, combining:

* Detection
* Tracking
* Segmentation
* Depth
* 3D reconstruction
* Stabilized 3D bounding boxes
* BEV pseudo-LiDAR mapping
* Exportable point clouds
